{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/15], Iter [10/15] Loss: 175.7288, Acc: 0.000000\n",
      "Epoch [1/15], Iter [20/15] Loss: 23.7623, Acc: 0.000000\n",
      "Epoch [1/15], Iter [30/15] Loss: 36.6576, Acc: 0.000000\n",
      "Epoch [1/15], Iter [40/15] Loss: 13.6182, Acc: 0.000000\n",
      "Epoch [1/15], Iter [50/15] Loss: 2.3508, Acc: 0.000000\n",
      "Epoch [1/15], Iter [60/15] Loss: 2.1028, Acc: 0.200000\n",
      "Epoch [1/15], Iter [70/15] Loss: 1.2715, Acc: 0.000000\n",
      "EPOCH LOSS: 462.9911\n",
      "Epoch [2/15], Iter [10/15] Loss: 1.4760, Acc: 0.000000\n",
      "Epoch [2/15], Iter [20/15] Loss: 0.4256, Acc: 0.000000\n",
      "Epoch [2/15], Iter [30/15] Loss: 0.6708, Acc: 0.000000\n",
      "Epoch [2/15], Iter [40/15] Loss: 0.8687, Acc: 0.000000\n",
      "Epoch [2/15], Iter [50/15] Loss: 0.2757, Acc: 0.000000\n",
      "Epoch [2/15], Iter [60/15] Loss: 1.5095, Acc: 0.000000\n",
      "Epoch [2/15], Iter [70/15] Loss: 0.6418, Acc: 0.000000\n",
      "EPOCH LOSS: 0.6705\n",
      "Epoch [3/15], Iter [10/15] Loss: 0.3155, Acc: 0.000000\n",
      "Epoch [3/15], Iter [20/15] Loss: 1.5016, Acc: 0.000000\n",
      "Epoch [3/15], Iter [30/15] Loss: 0.7631, Acc: 0.000000\n",
      "Epoch [3/15], Iter [40/15] Loss: 0.4596, Acc: 0.000000\n",
      "Epoch [3/15], Iter [50/15] Loss: 0.5590, Acc: 0.000000\n",
      "Epoch [3/15], Iter [60/15] Loss: 0.1566, Acc: 0.000000\n",
      "Epoch [3/15], Iter [70/15] Loss: 0.8774, Acc: 0.000000\n",
      "EPOCH LOSS: 0.5899\n",
      "Epoch [4/15], Iter [10/15] Loss: 0.3552, Acc: 0.000000\n",
      "Epoch [4/15], Iter [20/15] Loss: 0.4487, Acc: 0.000000\n",
      "Epoch [4/15], Iter [30/15] Loss: 0.4041, Acc: 0.000000\n",
      "Epoch [4/15], Iter [40/15] Loss: 0.5987, Acc: 0.000000\n",
      "Epoch [4/15], Iter [50/15] Loss: 0.5917, Acc: 0.000000\n",
      "Epoch [4/15], Iter [60/15] Loss: 1.0671, Acc: 0.000000\n",
      "Epoch [4/15], Iter [70/15] Loss: 0.4036, Acc: 0.000000\n",
      "EPOCH LOSS: 0.5842\n",
      "Epoch [5/15], Iter [10/15] Loss: 0.3709, Acc: 0.000000\n",
      "Epoch [5/15], Iter [20/15] Loss: 0.4194, Acc: 0.000000\n",
      "Epoch [5/15], Iter [30/15] Loss: 0.6900, Acc: 0.000000\n",
      "Epoch [5/15], Iter [40/15] Loss: 0.2251, Acc: 0.000000\n",
      "Epoch [5/15], Iter [50/15] Loss: 0.3725, Acc: 0.000000\n",
      "Epoch [5/15], Iter [60/15] Loss: 0.3417, Acc: 0.000000\n",
      "Epoch [5/15], Iter [70/15] Loss: 0.8630, Acc: 0.000000\n",
      "EPOCH LOSS: 0.5840\n",
      "Epoch [6/15], Iter [10/15] Loss: 0.7856, Acc: 0.000000\n",
      "Epoch [6/15], Iter [20/15] Loss: 0.3954, Acc: 0.000000\n",
      "Epoch [6/15], Iter [30/15] Loss: 0.2941, Acc: 0.000000\n",
      "Epoch [6/15], Iter [40/15] Loss: 0.4465, Acc: 0.000000\n",
      "Epoch [6/15], Iter [50/15] Loss: 1.0203, Acc: 0.000000\n",
      "Epoch [6/15], Iter [60/15] Loss: 0.6838, Acc: 0.000000\n",
      "Epoch [6/15], Iter [70/15] Loss: 0.8432, Acc: 0.000000\n",
      "EPOCH LOSS: 0.5713\n",
      "Epoch [7/15], Iter [10/15] Loss: 0.4942, Acc: 0.000000\n",
      "Epoch [7/15], Iter [20/15] Loss: 0.8691, Acc: 0.000000\n",
      "Epoch [7/15], Iter [30/15] Loss: 0.5569, Acc: 0.000000\n",
      "Epoch [7/15], Iter [40/15] Loss: 0.0892, Acc: 0.000000\n",
      "Epoch [7/15], Iter [50/15] Loss: 0.2506, Acc: 0.000000\n",
      "Epoch [7/15], Iter [60/15] Loss: 0.4111, Acc: 0.000000\n",
      "Epoch [7/15], Iter [70/15] Loss: 0.4043, Acc: 0.000000\n",
      "EPOCH LOSS: 0.5834\n",
      "Epoch [8/15], Iter [10/15] Loss: 0.0590, Acc: 0.000000\n",
      "Epoch [8/15], Iter [20/15] Loss: 1.0704, Acc: 0.000000\n",
      "Epoch [8/15], Iter [30/15] Loss: 0.3842, Acc: 0.000000\n",
      "Epoch [8/15], Iter [40/15] Loss: 0.9554, Acc: 0.000000\n",
      "Epoch [8/15], Iter [50/15] Loss: 0.5652, Acc: 0.000000\n",
      "Epoch [8/15], Iter [60/15] Loss: 1.0513, Acc: 0.000000\n",
      "Epoch [8/15], Iter [70/15] Loss: 0.4902, Acc: 0.000000\n",
      "EPOCH LOSS: 0.5780\n",
      "Epoch [9/15], Iter [10/15] Loss: 0.3623, Acc: 0.000000\n",
      "Epoch [9/15], Iter [20/15] Loss: 0.6655, Acc: 0.000000\n",
      "Epoch [9/15], Iter [30/15] Loss: 0.7339, Acc: 0.000000\n",
      "Epoch [9/15], Iter [40/15] Loss: 0.3454, Acc: 0.000000\n",
      "Epoch [9/15], Iter [50/15] Loss: 0.9073, Acc: 0.000000\n",
      "Epoch [9/15], Iter [60/15] Loss: 0.5477, Acc: 0.000000\n",
      "Epoch [9/15], Iter [70/15] Loss: 0.8300, Acc: 0.000000\n",
      "EPOCH LOSS: 0.5721\n",
      "Epoch [10/15], Iter [10/15] Loss: 1.1544, Acc: 0.000000\n",
      "Epoch [10/15], Iter [20/15] Loss: 0.6793, Acc: 0.000000\n",
      "Epoch [10/15], Iter [30/15] Loss: 0.2912, Acc: 0.000000\n",
      "Epoch [10/15], Iter [40/15] Loss: 1.1908, Acc: 0.000000\n",
      "Epoch [10/15], Iter [50/15] Loss: 0.4283, Acc: 0.000000\n",
      "Epoch [10/15], Iter [60/15] Loss: 0.1682, Acc: 0.000000\n",
      "Epoch [10/15], Iter [70/15] Loss: 0.4808, Acc: 0.000000\n",
      "EPOCH LOSS: 0.5766\n",
      "Epoch [11/15], Iter [10/15] Loss: 0.4175, Acc: 0.000000\n",
      "Epoch [11/15], Iter [20/15] Loss: 0.6008, Acc: 0.000000\n",
      "Epoch [11/15], Iter [30/15] Loss: 0.2910, Acc: 0.000000\n",
      "Epoch [11/15], Iter [40/15] Loss: 1.3819, Acc: 0.000000\n",
      "Epoch [11/15], Iter [50/15] Loss: 0.6765, Acc: 0.000000\n",
      "Epoch [11/15], Iter [60/15] Loss: 0.7045, Acc: 0.000000\n",
      "Epoch [11/15], Iter [70/15] Loss: 0.6088, Acc: 0.000000\n",
      "EPOCH LOSS: 0.5739\n",
      "Epoch [12/15], Iter [10/15] Loss: 0.9344, Acc: 0.000000\n",
      "Epoch [12/15], Iter [20/15] Loss: 0.7754, Acc: 0.000000\n",
      "Epoch [12/15], Iter [30/15] Loss: 0.4658, Acc: 0.000000\n",
      "Epoch [12/15], Iter [40/15] Loss: 0.8084, Acc: 0.000000\n",
      "Epoch [12/15], Iter [50/15] Loss: 0.5648, Acc: 0.000000\n",
      "Epoch [12/15], Iter [60/15] Loss: 0.7190, Acc: 0.000000\n",
      "Epoch [12/15], Iter [70/15] Loss: 1.7082, Acc: 0.000000\n",
      "EPOCH LOSS: 0.5890\n",
      "Epoch [13/15], Iter [10/15] Loss: 0.5800, Acc: 0.000000\n",
      "Epoch [13/15], Iter [20/15] Loss: 0.1722, Acc: 0.000000\n",
      "Epoch [13/15], Iter [30/15] Loss: 0.0787, Acc: 0.000000\n",
      "Epoch [13/15], Iter [40/15] Loss: 0.7958, Acc: 0.000000\n",
      "Epoch [13/15], Iter [50/15] Loss: 0.4576, Acc: 0.000000\n",
      "Epoch [13/15], Iter [60/15] Loss: 0.8388, Acc: 0.000000\n",
      "Epoch [13/15], Iter [70/15] Loss: 0.3013, Acc: 0.000000\n",
      "EPOCH LOSS: 0.6017\n",
      "Epoch [14/15], Iter [10/15] Loss: 0.1651, Acc: 0.000000\n",
      "Epoch [14/15], Iter [20/15] Loss: 0.5974, Acc: 0.000000\n",
      "Epoch [14/15], Iter [30/15] Loss: 0.5968, Acc: 0.000000\n",
      "Epoch [14/15], Iter [40/15] Loss: 0.5699, Acc: 0.000000\n",
      "Epoch [14/15], Iter [50/15] Loss: 0.1685, Acc: 0.000000\n",
      "Epoch [14/15], Iter [60/15] Loss: 1.7474, Acc: 0.000000\n",
      "Epoch [14/15], Iter [70/15] Loss: 0.7334, Acc: 0.000000\n",
      "EPOCH LOSS: 0.5931\n",
      "Epoch [15/15], Iter [10/15] Loss: 0.2777, Acc: 0.000000\n",
      "Epoch [15/15], Iter [20/15] Loss: 0.4770, Acc: 0.000000\n",
      "Epoch [15/15], Iter [30/15] Loss: 0.6089, Acc: 0.000000\n",
      "Epoch [15/15], Iter [40/15] Loss: 0.3595, Acc: 0.000000\n",
      "Epoch [15/15], Iter [50/15] Loss: 0.0649, Acc: 0.000000\n",
      "Epoch [15/15], Iter [60/15] Loss: 1.0472, Acc: 0.000000\n",
      "Epoch [15/15], Iter [70/15] Loss: 1.3909, Acc: 0.000000\n",
      "EPOCH LOSS: 0.5785\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import torch.autograd\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import data_feeder as df\n",
    "import numpy\n",
    "\n",
    "\n",
    "\n",
    "class Model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Model, self).__init__()\n",
    "        pretrained = False;\n",
    "        if (pretrained == True):\n",
    "            self.model = torchvision.models.vgg16(pretrained=True)\n",
    "            for param in self.model.parameters():\n",
    "                param.requires_grad = False\n",
    "            self.model.fc = nn.Linear(128, 1) \n",
    "        else:\n",
    "            self.model = nn.Sequential(\n",
    "                    nn.Conv2d(3, 8, 5, stride = 2) \n",
    "                    ,nn.BatchNorm2d(8)\n",
    "                        ,nn.ReLU()\n",
    "                        ,nn.Conv2d(8, 16, 5, stride = 2)\n",
    "                        ,nn.BatchNorm2d(16)\n",
    "                            ,nn.ReLU()\n",
    "                        ,nn.Conv2d(16, 32, 5)\n",
    "                            ,nn.BatchNorm2d(32)\n",
    "                            ,nn.ReLU()\n",
    "                            ,nn.Conv2d(32, 48, 5)\n",
    "                            ,nn.BatchNorm2d(48)\n",
    "                            ,nn.ReLU()\n",
    "                            ,nn.Conv2d(48, 1, 5)\n",
    "                )\n",
    "            \n",
    "           # self.fc1 = nn.Linear(24, 64)\n",
    "           # self.fc2 = nn.Linear(64, 1)\n",
    "              #                  ,nn.Linear(48 * 48 * 24, 128)\n",
    "             #                       ,nn.BatchNorm1d(128)\n",
    "            #                        ,nn.ReLU()\n",
    "           #                         ,nn.Linear(128, 1)\n",
    "                \n",
    "    \n",
    "    def forward(self, input_data):\n",
    "        return self.model(input_data)\n",
    "    \n",
    " #   def _bias_init(self, b):\n",
    "  #      b.bias = torch.nn.Parameter(torch.zeros(len(b.bias)))\n",
    "\n",
    "  #  def _weight_init(self, m):\n",
    "   #     size = m.weight.size()\n",
    "    #    fan_out = size[0]  # number of rows\n",
    "    #    fan_in = size[1]  # number of columns\n",
    "    #    variance = np.sqrt(2.0 / (fan_in + fan_out))\n",
    "    #    m.weight.data.normal_(0.0, variance)\n",
    "    @property\n",
    "    def is_cuda(self):\n",
    "        return next(self.parameters()).is_cuda\n",
    "    \n",
    "class Solver():\n",
    "    def __init__(self):\n",
    "        super(Solver, self).__init__()\n",
    "        \n",
    "    def loss(self, pred, labels):\n",
    "        return (pred - labels).pow(2).sum() / len(labels)\n",
    "    \n",
    "    def accuracy(self, pred, labels):\n",
    "        acc = 0\n",
    "        for i in range(len(labels)):\n",
    "            predict = pred[i].data.cpu().numpy()[0]\n",
    "            begin = labels[i].data.cpu().numpy() * 1.1\n",
    "            end = labels[i].data.cpu().numpy() * 0.9\n",
    "          #  print(predict)\n",
    "            if predict >= 0:\n",
    "                if predict >= end and predict <= begin:\n",
    "                    acc+=1\n",
    "            else:\n",
    "                if predict <= end and predict >= begin:\n",
    "                    acc+=1\n",
    "                \n",
    "        return acc/len(labels)\n",
    "    \n",
    "    def train(self, train_data, num_epochs = 15, learning_rate = 1e-1, batch_size = 5, pretrained = False):\n",
    "\n",
    "            train_loader = torch.utils.data.DataLoader(dataset = train_data,\n",
    "\n",
    "                                               batch_size=batch_size, \n",
    "\n",
    "                                               shuffle=True, num_workers = 0)\n",
    "       #    print(len(train_loader))\n",
    "            cnn = Model()\n",
    "            optimizer = torch.optim.Adam(cnn.parameters(), lr = learning_rate)\n",
    "            loss_history = []\n",
    "            if torch.cuda.is_available():\n",
    "                cnn.cuda()\n",
    "            for i in range(num_epochs):\n",
    "                for j, (images, labels) in enumerate(train_loader):\n",
    "                    images = Variable(images.float())\n",
    "                    labels = Variable(labels.float())\n",
    "                    labels *= 5\n",
    "                    if cnn.is_cuda:\n",
    "                        inputs, targets = inputs.cuda(), targets.cuda()\n",
    "                    optimizer.zero_grad()\n",
    "                    pred = cnn(images)\n",
    "                    pred *= 5\n",
    "                   # print(pred)\n",
    "                    loss = self.loss(pred, labels)\n",
    "                    loss_history.append(loss)\n",
    "                    cnn.zero_grad()\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "                       \n",
    "                    if (j+1) % 10 == 0:\n",
    "\n",
    "                        print ('Epoch [%d/%d], Iter [%d/%d] Loss: %.4f, Acc: %4f' \n",
    "\n",
    "                           %(i+1, num_epochs, j+1, len(train_loader)//batch_size, loss.data[0], self.accuracy(pred, labels)))\n",
    "                epoch_loss = 0\n",
    "                for j in range(1, len(train_loader)):\n",
    "                    epoch_loss += loss_history[-j]\n",
    "                print ('EPOCH LOSS: %.4f' % (epoch_loss/len(train_loader)))\n",
    "                       # print(pred, labels)\n",
    "                        \n",
    "train_data, val_data = df.getDrivingData('race1516207298.txt')\n",
    "\n",
    "solver = Solver()\n",
    "solver.train(train_data)         \n",
    "    \n",
    "        \n",
    "    \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
